{
  "cells": [
    {
      "metadata": {
        "_uuid": "225708f447eee93041881f9d6c3a3e890cb16718"
      },
      "cell_type": "markdown",
      "source": "**XGboost with joined data**[](http://)"
    },
    {
      "metadata": {
        "_uuid": "2c996d745ce4e3d8516bf52e9bea948c37a3b971"
      },
      "cell_type": "markdown",
      "source": "# Loading market data and news data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cf24e5e4916804424a07bb0a94261ce2599f752"
      },
      "cell_type": "code",
      "source": "# Will reduce data load for code test\ntoy = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e349943cc34c29c4e9605d293e9986d5e1e9bc3"
      },
      "cell_type": "code",
      "source": "market_train_df.shape, news_train_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fba8c359ca9f08fe0f0ab824c325ed9ab77c9e63"
      },
      "cell_type": "code",
      "source": "# We will reduce the number of samples for memory reasons\nif toy:\n    market_train_df = market_train_df.tail(100_000)\n    news_train_df = news_train_df.tail(300_000)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b092c146da2a745bf03866d22467b09a83b2730a"
      },
      "cell_type": "code",
      "source": "import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "747c4c9eee1ec2e925d2329b3b2dcc55d019046b"
      },
      "cell_type": "markdown",
      "source": "# Joining market data and news data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8174e0fe351b1c2c667fb463406975e941aba9c"
      },
      "cell_type": "code",
      "source": "news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c5a87dde9b7931382da32d0b362116fb8fbc94c"
      },
      "cell_type": "code",
      "source": "def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e2ac99f812da232b9b46e4021e8b0fc79048e4d"
      },
      "cell_type": "markdown",
      "source": "# Splitting joined dataset into trainning data and testing data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "969511254abbe99bfbb38c4e85422596635cbd30"
      },
      "cell_type": "code",
      "source": "def get_xy(market_train_df, news_train_df, le=None):\n    x, le = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10']>=0\n    return x, y, le\n\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\ndef get_x(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n#    x.fillna(-1000,inplace=True)\n\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        x[bogus_col] = x[bogus_col].astype(float)\n    \n    return x, (le_assetCode, le_assetName)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31cd42303a383d2a6537c6f7b99ec2d298f26ff7"
      },
      "cell_type": "code",
      "source": "%%time\n\n# This will take some time...\nX, y, le = get_xy(market_train_df, news_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "969d2f159c075d73660850d143d7fd1a19fa0915"
      },
      "cell_type": "code",
      "source": "X.shape, y.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "068cd43c59e905593126201737f8a82d1024cdfb"
      },
      "cell_type": "code",
      "source": "# Save universe data for latter use\nu= market_train_df['universe']\nday = market_train_df['time']\nr=market_train_df['returnsOpenNextMktres10'].clip(-1,1)\n# Free memory\ndel market_train_df, news_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5142e49fb4156dddd15740beb4d38ecd23398247"
      },
      "cell_type": "code",
      "source": "X_ = X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "76afa018beace8447edbe3ccd904aa8b8b772e4c"
      },
      "cell_type": "code",
      "source": "# Keep only text columns\nX = X_#.iloc[:, X.columns.get_loc('urgency_min'):X.columns.get_loc('dayofweek')]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ffccbc2cf61684731578abd3e1108bff691c23a0"
      },
      "cell_type": "code",
      "source": "X.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "676b28bbf482d680e642c6c7450bf5c918399467"
      },
      "cell_type": "code",
      "source": "n_train = int(X.shape[0] * 0.8)\n\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8a172258a46c7db96e85964d6d56c24f0ec56f0c"
      },
      "cell_type": "code",
      "source": "# For valid data, keep only those with universe > 0. This will help calculate the metric\nd_valid = day.iloc[n_train:]\nr_valid=r.iloc[n_train:]\nu_valid=u.iloc[n_train:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e930607ae5b390f3dcd0a5a420dac5c22148d96"
      },
      "cell_type": "code",
      "source": "len(X_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5c36575670523810d49d7581f74d2d24ece8f82a"
      },
      "cell_type": "markdown",
      "source": "# Using XGBoost for modeling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0db2ae5af40fc339bbbd0f9200e96e83b540cef6"
      },
      "cell_type": "code",
      "source": "import time\n#from other kernel\nfrom xgboost import XGBClassifier\nxgb_market = XGBClassifier(n_jobs=4, n_estimators=200, max_depth=8, eta=0.1)\n#xgb_market = XGBClassifier(n_jobs=4, n_estimators=100)\nt = time.time()\nprint('Fitting Up')\nxgb_market.fit(X_train,y_train)\nprint(f'Done, time = {time.time() - t}s')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da195907a88dcdc7b2b6e7724accf4b348793329"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n# distribution of confidence that will be used as submission\nconfidence_test = xgb_market.predict_proba(X_valid)[:,1]*2 -1\nprint(accuracy_score(confidence_test>0,y_valid))\nplt.hist(confidence_test, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2db03252e8057ebfd31dcc783dcb7b7433be939b"
      },
      "cell_type": "markdown",
      "source": "# Calculating the accuracy score"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93b32b9ae1fa1bc0acfb26bf45c6274a1bea46d4"
      },
      "cell_type": "code",
      "source": "# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_test * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean / std\nprint(score_test)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "631e4711f0c8de6df10b297dee6b1697afe4259b"
      },
      "cell_type": "code",
      "source": "days = env.get_prediction_days()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2d8d939046bfcbee3fe62255ae83f59ea000850"
      },
      "cell_type": "code",
      "source": "n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n    # discard assets that are not scored\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X_market_obs = prep_data(market_obs_df)[0]\n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = xgb_market.predict_proba(X_market_obs)[:,1]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}